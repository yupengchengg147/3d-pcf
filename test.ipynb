{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tde = TD_Encoder(16)\n",
    "td = 1.0* torch.ones(size=(1024, 1))\n",
    "out = tde(td)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2439,  0.1176, -0.0426,  0.0015, -0.1101,  0.1576, -0.0535, -0.0309,\n",
       "          0.0995,  0.0938,  0.0742,  0.0276, -0.0276, -0.0258, -0.1131, -0.0764],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([-0.2439,  0.1176, -0.0426,  0.0015, -0.1101,  0.1576, -0.0535, -0.0309,\n",
       "          0.0995,  0.0938,  0.0742,  0.0276, -0.0276, -0.0258, -0.1131, -0.0764],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0], out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = out[:,-1]\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 17])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ro = torch.cat((out, r.unsqueeze(-1)), dim=-1)\n",
    "ro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_cluster import knn_graph, knn\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius, global_mean_pool\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## oth test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mp 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo data\n",
    "bpos_1 = torch.randn(2*1024,3)\n",
    "bpos_5 =torch.randn(2*1024,3)\n",
    "bfea_1 = torch.randn(2*1024,24)\n",
    "bfea_5 =torch.randn(2*1024,24)\n",
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
       "        [ 674,  691,  514,  ..., 1390, 1777, 1802]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds for each element in :obj:`y` the :obj:`k` nearest points in :obj:`x`.\n",
    "index_ex = knn(x=bpos_1,y=bpos_5, k=10, batch_x=dbatch, batch_y=dbatch)\n",
    "index_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 417,  438,  384,  ..., 1115, 1155, 1887],\n",
       "        [   0,    0,    0,  ..., 2047, 2047, 2047]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_in = knn_graph(bpos_5, k=10, batch=dbatch, loop=False)\n",
    "index_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_id_source, in_id_target =  index_in[0], index_in[1]\n",
    "\n",
    "ex_id_source, ex_id_target =  index_ex[1], index_ex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local_Point_Trans(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lin_p = nn.Linear(4, 256)\n",
    "        self.lin_q = nn.Linear(256, 256)\n",
    "        self.lin_k = nn.Linear(256, 256)\n",
    "        self.lin_v = nn.Linear(256, 256)\n",
    "        self.lin_w = nn.Linear(256, 256)\n",
    "        self.att = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, fea_j, fea_i, xyzt_j, xyzt_i):\n",
    "        \"\"\"\n",
    "        fea_j: [N, k, 256]\n",
    "        fea_i: [N, k, 256]\n",
    "        xyzt_j: [N, k, 4]\n",
    "        xyzt_i: [N, k, 4]\n",
    "        \"\"\" \n",
    "\n",
    "        pe = self.lin_p(xyzt_j - xyzt_i)  # [N, k, 256]\n",
    "\n",
    "        w = self.lin_w(self.lin_q(fea_i) - self.lin_k(fea_j) + pe) # [N, k, 256]\n",
    "\n",
    "        w = self.att(w)\n",
    "\n",
    "        print(w.sum(dim=1)[0]) #[1,1,...,1] 256dim\n",
    "\n",
    "        v = self.lin_v(fea_j) + pe\n",
    "\n",
    "        print(v.shape)\n",
    "        print(w.shape)\n",
    "\n",
    "        return w, v\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SelectBackward0>)\n",
      "torch.Size([100, 20, 256])\n",
      "torch.Size([100, 20, 256])\n"
     ]
    }
   ],
   "source": [
    "fea_i = torch.randn(100,20,256)\n",
    "xyzt_i = torch.randn(100,20, 4)\n",
    "fea_j = torch.randn(100,20, 256)\n",
    "xyzt_j = torch.randn(100,20, 4)\n",
    "\n",
    "lpt = Local_Point_Trans()\n",
    "w, v = lpt(fea_j, fea_i, xyzt_j, xyzt_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpos_1 = torch.randn(2*1024,3)\n",
    "bpos_5 =torch.randn(2*1024,3)\n",
    "bfea_1 = torch.randn(2*1024,24)\n",
    "bfea_5 =torch.randn(2*1024,24)\n",
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fps test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "def original_fps(xyz, npoint):\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def new_fps(xyz, npoint):\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "\n",
    "    ptr = torch.repeat_interleave(\n",
    "        torch.tensor([N*i for i in range(B)],requires_grad=False, dtype=torch.long), \n",
    "        npoint\n",
    "    ).to(device)\n",
    "    batch_x = torch.repeat_interleave(torch.tensor(range(B),requires_grad=False, dtype=torch.long), \n",
    "                                      N).to(device)\n",
    "    \n",
    "    ratio_x = float(npoint/N)\n",
    "\n",
    "    x = xyz.reshape(-1,C)\n",
    "\n",
    "    x_idx = fps(x, batch_x, ratio_x)\n",
    "    x_idx = x_idx - ptr\n",
    "\n",
    "    centroids = x_idx.reshape(B, npoint).long().to(device)\n",
    "    return centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1, 15, 11,  5, 27,  7, 32, 22, 10, 31],\n",
       "         [ 6, 24, 31, 34, 20,  8, 37, 14,  2, 12]]),\n",
       " tensor([[ 9, 32, 30, 13,  0, 38,  2, 20, 26,  3],\n",
       "         [ 2, 12,  3, 32, 16, 18, 30, 24, 33,  6]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "bpos_1 = torch.randn(2,40,3)\n",
    "n_p = 10\n",
    "\n",
    "ce_o = original_fps(bpos_1, n_p)\n",
    "ce_n = new_fps(bpos_1, n_p)\n",
    "ce_o, ce_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pointnet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.kitti_dataset_v2 import KittiDataset_2\n",
    "data_config = {'root': \"/home/stud/ding/PC_FC/PC_forecasting/kittiraw/dataset/sequences\", 'npoints': 4096*4, 'input_num': 5, 'pred_num': 5, 'tr_seqs': ['00']}\n",
    "demo_dataset = KittiDataset_2(root=data_config['root'], npoints=data_config['npoints'], input_num=data_config['input_num'], pred_num=data_config['pred_num'], seqs=data_config['tr_seqs'])\n",
    "in_xyz_list, in_fea_list, gt_xyz_list = demo_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 16384]), torch.Size([1, 16384]), torch.Size([3, 16384]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_xyz_list[0].shape, in_fea_list[0].shape,gt_xyz_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(demo_dataset, batch_size=2) # collate_fn=custom_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 16384]), torch.Size([2, 1, 16384]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_xyz, bin_fea, bgt_xyz = next(iter(train_dataloader))\n",
    "xyz, fea = bin_xyz[0], bin_fea[0]\n",
    "xyz.shape, fea.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoder_pn2 import *\n",
    "pnt_encoder = PNT_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1024]), torch.Size([2, 256, 1024]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_xyz, en_fea = pnt_encoder(xyz, fea)\n",
    "en_xyz.shape, en_fea.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = en_fea.permute(0,2,1).reshape(-1,256)\n",
    "\n",
    "aa.reshape(2,-1,256).permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpos_1 = torch.randn(2*1024,3)\n",
    "bpos_5 =torch.randn(2*1024,3)\n",
    "bfea_1 = torch.randn(2*1024,256)\n",
    "bfea_5 =torch.randn(2*1024,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
       "        [ 177,  760,  297,  ..., 1173, 1834, 1262]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 1024)\n",
    "by_to_x_index = knn(x=bpos_1,y=bpos_5, k=4, batch_x=dbatch, batch_y=dbatch)\n",
    "by_to_x_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_btw_fr = GATv2Conv(in_channels=(256,256),out_channels=256, heads=4, concat=False, negative_slope=0.2, bias=False, add_self_loops=False, flow='target_to_source')\n",
    "out = gat_btw_fr((bfea_5, bfea_1),by_to_x_index)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n",
    "    Compute the dot products of the query with all keys, divide each by sqrt(dim),\n",
    "    and apply a softmax function to obtain the weights on the values\n",
    "\n",
    "    Args: dim, mask\n",
    "        dim (int): dimention of attention\n",
    "        mask (torch.Tensor): tensor containing indices to be masked\n",
    "\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n",
    "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "        - **mask** (-): tensor containing indices to be masked\n",
    "\n",
    "    Returns: context, attn\n",
    "        - **context**: tensor containing the context vector from attention mechanism.\n",
    "        - **attn**: tensor containing the attention (alignment) from the encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention proposed in \"Attention Is All You Need\"\n",
    "    Instead of performing a single attention function with d_model-dimensional keys, values, and queries,\n",
    "    project the queries, keys and values h times with different, learned linear projections to d_head dimensions.\n",
    "    These are concatenated and once again projected, resulting in the final values.\n",
    "    Multi-head attention allows the model to jointly attend to information from different representation\n",
    "    subspaces at different positions.\n",
    "\n",
    "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) · W_o\n",
    "        where head_i = Attention(Q · W_q, K · W_k, V · W_v)\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of keys / values / quries (default: 512)\n",
    "        num_heads (int): The number of attention heads. (default: 8)\n",
    "\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): In transformer, three different ways:\n",
    "            Case 1: come from previoys decoder layer\n",
    "            Case 2: come from the input embedding\n",
    "            Case 3: come from the output embedding (masked)\n",
    "\n",
    "        - **key** (batch, k_len, d_model): In transformer, three different ways:\n",
    "            Case 1: come from the output of the encoder\n",
    "            Case 2: come from the input embeddings\n",
    "            Case 3: come from the output embedding (masked)\n",
    "\n",
    "        - **value** (batch, v_len, d_model): In transformer, three different ways:\n",
    "            Case 1: come from the output of the encoder\n",
    "            Case 2: come from the input embeddings\n",
    "            Case 3: come from the output embedding (masked)\n",
    "\n",
    "        - **mask** (-): tensor containing indices to be masked\n",
    "\n",
    "    Returns: output, attn\n",
    "        - **output** (batch, output_len, dimensions): tensor containing the attended output features.\n",
    "        - **attn** (batch * num_heads, v_len): tensor containing the attention (alignment) from the encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 512, num_heads: int = 8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n",
    "\n",
    "        self.d_head = int(d_model / num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.d_head)\n",
    "        self.query_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.key_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "        self.value_proj = nn.Linear(d_model, self.d_head * num_heads)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: Tensor,\n",
    "            key: Tensor,\n",
    "            value: Tensor,\n",
    "            mask: Optional[Tensor] = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        batch_size = value.size(0)\n",
    "\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)  # BxQ_LENxNxD\n",
    "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_head)      # BxK_LENxNxD\n",
    "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_head)  # BxV_LENxNxD\n",
    "\n",
    "        query = query.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxQ_LENxD\n",
    "        key = key.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)      # BNxK_LENxD\n",
    "        value = value.permute(2, 0, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.d_head)  # BNxV_LENxD\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # BxNxQ_LENxK_LEN\n",
    "\n",
    "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
    "\n",
    "        context = context.view(self.num_heads, batch_size, -1, self.d_head)\n",
    "        context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # BxTxND\n",
    "\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my attention agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agg_btw_frames(nn.Module):\n",
    "    def __init__(self,dim,num_heads, num_neighs):\n",
    "        super(Agg_btw_frames, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.m_att = MultiHeadAttention(dim, num_heads)\n",
    "        self.num_neighs = num_neighs\n",
    "    \n",
    "    def forward(self, fea_i, fea_j, batch, pos_i=None, pos_j=None):\n",
    "        \"\"\"\n",
    "        pos_i, j: (B*N, 3)\n",
    "        fea_i, fea_j: (B*N, dim)\n",
    "        batch: LongTensor(B*N)\n",
    "        eg: t = torch.LongTensor([0,1])\n",
    "            dbatch = torch.repeat_interleave(t, 1024)\n",
    "        \"\"\"\n",
    "        if (pos_i is not None) and (pos_j is not None):\n",
    "            y_to_x_index = knn(pos_i, pos_j, self.num_neighs,batch, batch)\n",
    "        else:\n",
    "            y_to_x_index = knn(fea_i, fea_j, self.num_neighs,batch, batch)\n",
    "\n",
    "        out = torch.empty_like(fea_j)\n",
    "        \n",
    "        for i in range(fea_j.shape[-2]):\n",
    "            f1_index_i = y_to_x_index[1][y_to_x_index[0,:]==i] # f5_i 根据 pos 找到的 10个f1中的neighbors\n",
    "            q_i = fea_j[i,:][None, None,:] # (1,1,dim)\n",
    "            k_i = fea_i[f1_index_i,:][None, :,:] # (1,k,dim)\n",
    "            v_i = fea_i[f1_index_i,:][None, :,:] # (1,k,dim)\n",
    "            context_i,_ = self.m_att(q_i, k_i, v_i) # (1,1,dim)\n",
    "            # print(context.shape)\n",
    "            out[i,:] = context_i.view(-1, self.dim)\n",
    "\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2, num_points = 1024, num_pred = 5\n",
    "\n",
    "bpos_1 = torch.randn(2*1024,3)\n",
    "bpos_5 =torch.randn(2*1024,3)\n",
    "bfea_1 = torch.randn(2*1024,24)\n",
    "bfea_5 =torch.randn(2*1024,24)\n",
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 24]),\n",
       " tensor([[ 0.0467,  0.2136, -0.0252,  ...,  0.1901,  0.2268,  0.1500],\n",
       "         [-0.3595,  0.3060,  0.2184,  ..., -0.3934,  0.3580,  0.4118],\n",
       "         [ 0.0872,  0.0186,  0.2714,  ...,  0.3095, -0.1507,  0.1483],\n",
       "         ...,\n",
       "         [ 0.3661, -0.0559, -0.1092,  ..., -0.1361,  0.2176,  0.0148],\n",
       "         [ 0.0114, -0.1161,  0.2120,  ...,  0.1526,  0.1066, -0.1007],\n",
       "         [ 0.3157,  0.1668,  0.2611,  ...,  0.0051,  0.5298,  0.0334]],\n",
       "        grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel = Agg_btw_frames(dim=24, num_heads=4, num_neighs=10)\n",
    "out = mymodel(bfea_1,bfea_5,dbatch,bpos_1, bpos_5)\n",
    "out.shape, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in mymodel.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {parameter.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpos_1 = torch.randn(2*1024,3)\n",
    "bpos_5 =torch.randn(2*1024,3)\n",
    "bfea_1 = torch.randn(2*1024,24)\n",
    "bfea_5 =torch.randn(2*1024,24)\n",
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " torch.Size([2, 20480]),\n",
       " tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
       "         [ 589,  256,  974,  ..., 1394, 1106, 1396]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_to_x_index = knn(x=bfea_1,y=bfea_5, k=10, batch_x=dbatch, batch_y=dbatch)\n",
    "type(by_to_x_index), by_to_x_index.shape, by_to_x_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1545, 1418, 1847, 1516, 1093, 1475, 1817, 1197, 2011, 1827]),\n",
       " tensor([  38,  234,  992,  826,    0,  142,  469, 1002,  619,  140]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_to_x_index[1][by_to_x_index[0,:]==1027], by_to_x_index[1][by_to_x_index[0,:]==104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ..., 199, 199, 199],\n",
       "        [ 76,  67,  30,  ..., 128, 144, 183]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfea_1 = torch.randn(2*100,24)\n",
    "bfea_5 =torch.randn(2*100,24)\n",
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 100)\n",
    "by_to_x_index = knn(x=bfea_1,y=bfea_5, k=4, batch_x=dbatch, batch_y=dbatch)\n",
    "by_to_x_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha torch.Size([800, 2])\n",
      "tensor([0.1248, 0.1533], grad_fn=<SelectBackward0>)\n",
      "out.shape:  torch.Size([200, 2, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 24])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat_btw_fr = GATv2Conv(in_channels=(24,24),out_channels=24, heads=2, concat=False, negative_slope=0.2, bias=False, add_self_loops=False, flow='target_to_source')\n",
    "out = gat_btw_fr((bfea_5, bfea_1),by_to_x_index)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some tests for aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_1 = torch.randn(1024,3)\n",
    "pos_5 =torch.randn(1024,3)\n",
    "fea_1 = torch.randn(1024,24)\n",
    "fea_5 = torch.randn(1024,24)\n",
    "y_to_x_index = knn(pos_1, pos_5, 10)\n",
    "y_to_x_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20480]),\n",
       " tensor([[   0,    0,    0,  ..., 2047, 2047, 2047],\n",
       "         [ 821,  775,  811,  ..., 1118, 1518, 1169]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpos_1 = torch.randn(2*1024,3)\n",
    "bpos_5 =torch.randn(2*1024,3)\n",
    "t = torch.LongTensor([0,1])\n",
    "dbatch = torch.repeat_interleave(t, 1024)\n",
    "\n",
    "by_to_x_index = knn(x=bpos_1,y=bpos_5, k=10, batch_x=dbatch, batch_y=dbatch)\n",
    "by_to_x_index.shape, by_to_x_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1325, 1795, 1399, 1340, 1600, 1892, 1537, 1282, 1617, 1191]),\n",
       " tensor([158, 479, 162, 761, 632, 689, 584, 462, 872, 497]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_to_x_index[1][by_to_x_index[0,:]==1027], by_to_x_index[1][by_to_x_index[0,:]==104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "f1_index_i = by_to_x_index[1][by_to_x_index[0,:]==i] # f5_i 根据 pos 找到的 10个f1中的neighbors\n",
    "\n",
    "q_i = bfea_5[i,:][None, None,:]\n",
    "print(q_i.shape)\n",
    "k_i = bfea_1[f1_index_i,:][None, :,:]\n",
    "v_i = bfea_1[f1_index_i,:][None, :,:]\n",
    "print(k_i.shape)\n",
    "\n",
    "f_1_5_attn = ScaledDotProductAttention(dim=24)\n",
    "context, attn = f_1_5_attn(q_i, k_i,v_i)\n",
    "context, attn, context.shape, attn.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "f1_index_i = y_to_x_index[1][y_to_x_index[0,:]==i] # f5_i 根据 pos 找到的 10个f1中的neighbors\n",
    "\n",
    "q_i = fea_5[i,:][None, None,:]\n",
    "print(q_i.shape)\n",
    "k_i = fea_1[f1_index_i,:][None, :,:]\n",
    "v_i = fea_1[f1_index_i,:][None, :,:]\n",
    "print(k_i.shape)\n",
    "\n",
    "f_1_5_attn = ScaledDotProductAttention(dim=24)\n",
    "context, attn = f_1_5_attn(q_i, k_i,v_i)\n",
    "context, attn, context.shape, attn.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1_5_m_attn = MultiHeadAttention(d_model=24, num_heads=4)\n",
    "m_context, m_attn = f_1_5_m_attn(q_i, k_i,v_i)\n",
    "m_context, m_attn, m_context.shape, m_attn.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in f_1_5_m_attn.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {parameter.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "pc_seq = []\n",
    "pc_pos_seq = []\n",
    "for _ in range(5):\n",
    "    t = torch.randn(2,1024,24)\n",
    "    f = torch.randn(2,1024,3)\n",
    "    pc_seq.append(t)\n",
    "    pc_pos_seq.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_seq = []\n",
    "for pc in pc_seq:\n",
    "    #global_f_extractor to be updated\n",
    "    pc_global_fea, _ = torch.max(pc, dim=-2)\n",
    "    fea_seq.append(pc_global_fea)\n",
    "\n",
    "q = fea_seq[-1][:, None, :] # (B, 1, C)\n",
    "k = torch.stack(fea_seq[0:-1],dim=-2) # (B, T-1, C)\n",
    "v = torch.stack(fea_seq[0:-1],dim=-2)  # (B, T-1, C)\n",
    "print(q.shape, k.shape, v.shape)\n",
    "\n",
    "mat = ScaledDotProductAttention(24)\n",
    "context, att = mat(q,k,v)\n",
    "context.shape, context, att.shape, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_agg_model = Agg_btw_frames(dim=24, num_heads=4, num_neighs=10)\n",
    "batch_size = 2\n",
    "\n",
    "t = torch.LongTensor(range(batch_size))\n",
    "dbatch = torch.repeat_interleave(t, 1024)\n",
    "\n",
    "res = torch.empty(batch_size,len(pc_pos_seq)-1, 1024, 24) #(2,4,1024,24)\n",
    "\n",
    "for i in range(len(pc_pos_seq)-1):\n",
    "    out = local_agg_model(pc_seq[i].view(-1,24), pc_seq[-1].view(-1,24), dbatch,pc_pos_seq[i].view(-1,3), pc_pos_seq[-1].view(-1,3))\n",
    "    print(out.shape)\n",
    "    res[:,i,:,:] = out.view(batch_size,1024, 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(att.shape)\n",
    "# print(att)\n",
    "fr_weight = att.permute(0,2,1).unsqueeze(-1)\n",
    "# print(fr_weight.shape)\n",
    "# print(fr_weight)\n",
    "weighted_res = torch.sum(torch.mul(res, fr_weight),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_seq[-1].shape, weighted_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fea = torch.cat([pc_seq[-1].unsqueeze(1), weighted_res.unsqueeze(1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fea.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义输入张量 t 和权重张量 weight\n",
    "t = torch.randn(2, 4, 1024, 24)\n",
    "weight = torch.randn(2, 1, 4)\n",
    "\n",
    "# 将权重张量的形状扩展为 (2, 4, 1, 1)\n",
    "weight_expanded = weight.unsqueeze(-1).unsqueeze(-1).squeeze(1)\n",
    "\n",
    "weighted = torch.mul(t, weight_expanded)\n",
    "print(weighted.shape)  # 输出 (2, 4, 1024, 24)\n",
    "\n",
    "# 对帧的维度求和，得到加权求和的结果\n",
    "aggregated = torch.sum(weighted, dim=1)\n",
    "\n",
    "print(aggregated.shape)  # 输出 (2, 1024, 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据 global feature 生成 的 frame_attention 聚合 res 中的每个frame\n",
    "frame_att = att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_args = {\n",
    "    'channels': 24,\n",
    "    'k': 20,\n",
    "    'dilations': (1,2),\n",
    "    'n_idgcn_blocks':2,\n",
    "    'n_dgcn_blocks':2,\n",
    "    'radio': 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_kargs = {\n",
    "\n",
    "    'use_radius_graph': False,\n",
    "    'use_bottleneck': True,\n",
    "    'use_pooling': True,\n",
    "    'use_residual': True,\n",
    "    'conv': 'edge',\n",
    "    'pool_type': 'mean',\n",
    "    'dynamic': False,\n",
    "    'hierarchical': True,\n",
    "    'ratio': 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pugcn_lib.feature_extractor import InceptionFeatureExtractor\n",
    "pc_encoder = InceptionFeatureExtractor(**encoder_args, **encoder_kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some config\n",
    "num_points = 1024*4\n",
    "\n",
    "batch_size = 2\n",
    "num_frames = 5\n",
    "global_fea_dim = 24\n",
    "local_fea_dim = 24\n",
    "num_heads = 4\n",
    "num_neighs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch tensor\n",
    "t = torch.LongTensor(range(batch_size))\n",
    "in_batch = torch.repeat_interleave(t, num_points)\n",
    "encoder_batch = torch.repeat_interleave(t, int(num_points * encoder_kargs['ratio']))\n",
    "in_batch.shape, encoder_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pc_seq = []\n",
    "for _ in range(num_frames):\n",
    "    pc = torch.randn(batch_size*num_points,3)\n",
    "    input_pc_seq.append(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_encoded_seq = []\n",
    "global_fea_seq = []\n",
    "for i,pc in enumerate(input_pc_seq):\n",
    "    pc_encoded = pc_encoder(x=pc, pos = pc, batch=in_batch, return_index = False)\n",
    "    print(pc_encoded.shape)\n",
    "    pc_encoded_seq.append(pc_encoded)\n",
    "\n",
    "    #TODO: some node level projection before pooling\n",
    "    pc_pooled = global_mean_pool(pc_encoded, batch=encoder_batch).unsqueeze(1)\n",
    "    print(pc_pooled.shape)\n",
    "    global_fea_seq.append(pc_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = global_fea_seq[-1] # (B, 1, C)\n",
    "k = torch.stack(global_fea_seq[0:-1],dim=-2).squeeze(1) # (B, T-1, C)\n",
    "v = torch.stack(global_fea_seq[0:-1],dim=-2).squeeze(1)  # (B, T-1, C)\n",
    "\n",
    "print(q.shape, k.shape, v.shape)\n",
    "\n",
    "mat = ScaledDotProductAttention(global_fea_dim)\n",
    "_, frame_level_att = mat(q,k,v)\n",
    "frame_level_att.shape, frame_level_att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_agg_model = Agg_btw_frames(dim=local_fea_dim, num_heads=num_heads, num_neighs=num_neighs)\n",
    "\n",
    "res = torch.empty(batch_size,len(pc_encoded_seq)-1, int(num_points * encoder_kargs['ratio']), local_fea_dim)\n",
    "print(res.shape)\n",
    "\n",
    "for i, pc_encoded in enumerate(pc_encoded_seq[:-1]):\n",
    "    out = local_agg_model(pc_encoded, pc_encoded_seq[-1], encoder_batch)\n",
    "    print(out.shape)\n",
    "    res[:,i,:,:] = out.view(batch_size,int(num_points * encoder_kargs['ratio']), local_fea_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_weight = frame_level_att.permute(0,2,1).unsqueeze(-1)\n",
    "weighted_res = torch.sum(torch.mul(res, fr_weight),dim=1)\n",
    "fr_weight.shape, weighted_res.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_output = torch.cat([pc_encoded_seq[-1], weighted_res.view(-1,local_fea_dim)], dim=-1)\n",
    "time_series_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what decoder see:\n",
    "time_series_output.shape, encoder_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upsampler and refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pugcn_lib.upsample import GeneralUpsampler\n",
    "from pugcn_lib.models import Refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 256]), torch.Size([4096]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_out = torch.randn(4096, 256)\n",
    "tm_pos = torch.randn(4096, 3)\n",
    "\n",
    "t = torch.LongTensor([0,1,2,3])\n",
    "encoder_batch = torch.repeat_interleave(t, 1024)\n",
    "\n",
    "tm_out.shape, encoder_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampler_kargs = {\n",
    "    \"upsampler\": \"nodeshuffle\",\n",
    "    \"in_channels\": 256,\n",
    "    \"out_channels\": 64,\n",
    "    \"k\": 40,\n",
    "    \"r\": 16, \n",
    "    \"conv\":\"gatv2\",\n",
    "    \"heads\":4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([65536, 64]), torch.Size([65536]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampler = GeneralUpsampler(**upsampler_kargs)\n",
    "upsampled_pc, up_batch = upsampler(x=tm_out,pos=tm_pos, batch=encoder_batch, return_batch=True)\n",
    "upsampled_pc.shape, up_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructor =  torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 24),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(24, 3),)\n",
    "pc_reconstructed = reconstructor(upsampled_pc)\n",
    "pc_reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner_kargs = {\n",
    "    \"in_channels\": 64,\n",
    "    \"out_channels\": 3,\n",
    "    \"k\": 30,\n",
    "    \"dilations\": (1,2),\n",
    "    \"add_points\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65536, 64])\n",
      "torch.Size([65536, 3])\n",
      "torch.Size([65536])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_refiner = Refiner(**refiner_kargs)\n",
    "print(upsampled_pc.shape)\n",
    "print(pc_reconstructed.shape)\n",
    "print(up_batch.shape)\n",
    "\n",
    "pc_f1 = pc_refiner(x=upsampled_pc, pos=pc_reconstructed, batch=up_batch)\n",
    "pc_f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pc = torch.randn(batch_size*num_points,3)\n",
    "t_pc_encoded = pc_encoder(x=t_pc, pos = t_pc, batch=in_batch, return_index = False)\n",
    "t_pc_encoded.shape\n",
    "\n",
    "#TODO: some projection before pooling\n",
    "\n",
    "pc_pooled = global_mean_pool(t_pc_encoded, batch=encoder_batch)\n",
    "\n",
    "pc_pooled.shape, pc_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2048,24)\n",
    "b = torch.randn(2048,24)\n",
    "\n",
    "\n",
    "t_out = local_agg_model(a,b,encoder_batch)\n",
    "t_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2048,24)\n",
    "b = torch.randn(2048,24)\n",
    "pa = torch.randn(2048,3)\n",
    "pb = torch.randn(2048,3)\n",
    "\n",
    "\n",
    "t_out = local_agg_model(a,b,encoder_batch,pa,pb)\n",
    "t_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.kitti_dataset import KittiDataset\n",
    "kdata = \"./kittiraw/dataset/sequences\"\n",
    "pc_seq_dataset = KittiDataset(root=kdata, npoints=4096, input_num=5, pred_num=5, seqs=['01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pc_seq_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4096, 3])\n",
      "torch.Size([5, 4096, 3])\n"
     ]
    }
   ],
   "source": [
    "input_pc, gt_out_pc = pc_seq_dataset[2]\n",
    "print(input_pc.shape)\n",
    "print(gt_out_pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        super(KittiLoader, self).__init__(dataset, batch_size=batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        a_list, b_list = zip(*batch)\n",
    "\n",
    "        a_batch = torch.cat(a_list, dim=1)\n",
    "        b_batch = torch.cat(b_list, dim=1)\n",
    "\n",
    "        # concatenated_batch = torch.cat((a_batch, b_batch), dim=1)\n",
    "\n",
    "        return a_batch, b_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_loader = KittiLoader(pc_seq_dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 8192, 3]), torch.Size([5, 8192, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pc_seq , gt_pc_seq = next(iter(k_loader))\n",
    "input_pc_seq.shape, gt_pc_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(input_pc_seq.shape == torch.Size([5, 4096, 3])), \"input_pc_seq shape error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_args = {'channels': 24, 'k': 20, 'dilations': (1,2), 'n_idgcn_blocks':2, 'n_dgcn_blocks':2, 'radio': 0.25}\n",
    "encoder_kargs = { 'use_radius_graph': False, 'use_bottleneck': True, 'use_pooling': True, 'use_residual': True, 'conv': 'edge', 'pool_type': 'mean',\n",
    "'dynamic': False, 'hierarchical': True,}\n",
    "dim_args = {'global_fea_dim': 120 }\n",
    "att_args = {'num_heads': 4, 'num_neighs': 10}\n",
    "upsampler_args = { \"upsampler\": \"nodeshuffle\", \"in_channels\": 48, \"out_channels\": 24, \"k\": 20, \"r\": 4}\n",
    "refiner_kargs = { \"in_channels\": 24, \"out_channels\": 3, \"k\": 20, \"dilations\": (1,2), \"add_points\": True }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.time_series_models import PC_forecasting_model_0_0\n",
    "\n",
    "model = PC_forecasting_model_0_0(encoder_args, encoder_kargs, dim_args, att_args, upsampler_args, refiner_kargs)\n",
    "batch_size = 2\n",
    "k_loader = KittiLoader(pc_seq_dataset, batch_size=2)\n",
    "input_pc_seq = next(iter(k_loader))[0]\n",
    "\n",
    "pred_pc_seq = model(input_pc_seq, batch_size,num_pred=5, device='cpu')\n",
    "for i, pc in enumerate(pred_pc_seq):\n",
    "    print(i, pc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## point_cloud vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xvfb missed! cannot use open3d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pugcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
